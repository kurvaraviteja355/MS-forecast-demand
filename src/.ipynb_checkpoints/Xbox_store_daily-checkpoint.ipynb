{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import time \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import fbprophet\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.functions import current_date\n",
    "from fbprophet import Prophet\n",
    "from reduce_mem_usage import reduce_mem_usage\n",
    "from continuous_data import round_target_column, create_store_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_5944b02c79788fa0db5b3a93728ca2bf NOW.\n"
     ]
    },
    {
     "ename": "CompileError",
     "evalue": "Command \"gcc -g -DDEBUG -DMS_WIN64 -O0 -Wall -Wstrict-prototypes -DBOOST_RESULT_OF_USE_TR1 -DBOOST_NO_DECLTYPE -DBOOST_DISABLE_ASSERTS -D__MSVCRT_VERSION__=0x1916 -IC:\\Users\\RAVITE~1.KUR\\AppData\\Local\\Temp\\pystan_7tzpyj8y -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\pystan -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\pystan\\stan\\src -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\pystan\\stan\\lib\\stan_math -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\pystan\\stan\\lib\\stan_math\\lib\\eigen_3.3.3 -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\pystan\\stan\\lib\\stan_math\\lib\\boost_1.69.0 -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\pystan\\stan\\lib\\stan_math\\lib\\sundials_4.1.0\\include -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\numpy\\core\\include -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\include -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\include -c C:\\Users\\RAVITE~1.KUR\\AppData\\Local\\Temp\\pystan_7tzpyj8y\\stanfit4anon_model_5944b02c79788fa0db5b3a93728ca2bf_3038101750471626612.cpp -o C:\\Users\\RAVITE~1.KUR\\AppData\\Local\\Temp\\pystan_7tzpyj8y\\users\\ravite~1.kur\\appdata\\local\\temp\\pystan_7tzpyj8y\\stanfit4anon_model_5944b02c79788fa0db5b3a93728ca2bf_3038101750471626612.o -O2 -ftemplate-depth-256 -Wno-unused-function -Wno-uninitialized -std=c++1y -D_hypot=hypot -pthread -fexceptions\" failed with exit status 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDistutilsExecError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\fb_prophet\\lib\\distutils\\cygwinccompiler.py\u001b[0m in \u001b[0;36m_compile\u001b[1;34m(self, obj, src, ext, cc_args, extra_postargs, pp_opts)\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m                 self.spawn(self.compiler_so + cc_args + [src, '-o', obj] +\n\u001b[0m\u001b[0;32m    173\u001b[0m                            extra_postargs)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\numpy\\distutils\\ccompiler.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(self, *args, **kw)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;31m# Py3k does not have unbound method anymore, MethodType does not work\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m     \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\numpy\\distutils\\ccompiler.py\u001b[0m in \u001b[0;36mCCompiler_spawn\u001b[1;34m(self, cmd, display, env)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m     raise DistutilsExecError('Command \"%s\" failed with exit status %d%s' %\n\u001b[0m\u001b[0;32m    181\u001b[0m                             (cmd, s, msg))\n",
      "\u001b[1;31mDistutilsExecError\u001b[0m: Command \"gcc -g -DDEBUG -DMS_WIN64 -O0 -Wall -Wstrict-prototypes -DBOOST_RESULT_OF_USE_TR1 -DBOOST_NO_DECLTYPE -DBOOST_DISABLE_ASSERTS -D__MSVCRT_VERSION__=0x1916 -IC:\\Users\\RAVITE~1.KUR\\AppData\\Local\\Temp\\pystan_7tzpyj8y -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\pystan -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\pystan\\stan\\src -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\pystan\\stan\\lib\\stan_math -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\pystan\\stan\\lib\\stan_math\\lib\\eigen_3.3.3 -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\pystan\\stan\\lib\\stan_math\\lib\\boost_1.69.0 -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\pystan\\stan\\lib\\stan_math\\lib\\sundials_4.1.0\\include -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\numpy\\core\\include -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\include -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\include -c C:\\Users\\RAVITE~1.KUR\\AppData\\Local\\Temp\\pystan_7tzpyj8y\\stanfit4anon_model_5944b02c79788fa0db5b3a93728ca2bf_3038101750471626612.cpp -o C:\\Users\\RAVITE~1.KUR\\AppData\\Local\\Temp\\pystan_7tzpyj8y\\users\\ravite~1.kur\\appdata\\local\\temp\\pystan_7tzpyj8y\\stanfit4anon_model_5944b02c79788fa0db5b3a93728ca2bf_3038101750471626612.o -O2 -ftemplate-depth-256 -Wno-unused-function -Wno-uninitialized -std=c++1y -D_hypot=hypot -pthread -fexceptions\" failed with exit status 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCompileError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RAVITE~1.KUR\\AppData\\Local\\Temp/ipykernel_25916/2648765394.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpystan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'parameters {real y;} model {y ~ normal(0,1);}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpystan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStanModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_code\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# this will take a minute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# should be close to 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\pystan\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, charset, model_name, model_code, stanc_ret, include_paths, boost_lib, eigen_lib, verbose, obfuscate_model_name, extra_compile_args, allow_undefined, include_dirs, includes)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m             \u001b[0mbuild_extension\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mredirect_stderr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fb_prophet\\lib\\distutils\\command\\build_ext.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;31m# Now actually compile and link everything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_extensions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_extensions_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fb_prophet\\lib\\distutils\\command\\build_ext.py\u001b[0m in \u001b[0;36mbuild_extensions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_extensions_parallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_extensions_serial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_build_extensions_parallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fb_prophet\\lib\\distutils\\command\\build_ext.py\u001b[0m in \u001b[0;36m_build_extensions_serial\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    472\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextensions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filter_build_errors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fb_prophet\\lib\\distutils\\command\\build_ext.py\u001b[0m in \u001b[0;36mbuild_extension\u001b[1;34m(self, ext)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mmacros\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mundef\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m         objects = self.compiler.compile(sources,\n\u001b[0m\u001b[0;32m    529\u001b[0m                                          \u001b[0moutput_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_temp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m                                          \u001b[0mmacros\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmacros\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\numpy\\distutils\\ccompiler.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(self, *args, **kw)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreplace_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;31m# Py3k does not have unbound method anymore, MethodType does not work\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m     \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\numpy\\distutils\\ccompiler.py\u001b[0m in \u001b[0;36mCCompiler_compile\u001b[1;34m(self, sources, output_dir, macros, include_dirs, debug, extra_preargs, extra_postargs, depends)\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[1;31m# build serial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbuild_items\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m             \u001b[0msingle_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[1;31m# Return *all* object filenames, not just the ones we just built.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\numpy\\distutils\\ccompiler.py\u001b[0m in \u001b[0;36msingle_compile\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[1;31m# retrieve slot from our #job semaphore and build\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_job_semaphore\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra_postargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpp_opts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[1;31m# register being done processing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fb_prophet\\lib\\distutils\\cygwinccompiler.py\u001b[0m in \u001b[0;36m_compile\u001b[1;34m(self, obj, src, ext, cc_args, extra_postargs, pp_opts)\u001b[0m\n\u001b[0;32m    173\u001b[0m                            extra_postargs)\n\u001b[0;32m    174\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mDistutilsExecError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mCompileError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     def link(self, target_desc, objects, output_filename, output_dir=None,\n",
      "\u001b[1;31mCompileError\u001b[0m: Command \"gcc -g -DDEBUG -DMS_WIN64 -O0 -Wall -Wstrict-prototypes -DBOOST_RESULT_OF_USE_TR1 -DBOOST_NO_DECLTYPE -DBOOST_DISABLE_ASSERTS -D__MSVCRT_VERSION__=0x1916 -IC:\\Users\\RAVITE~1.KUR\\AppData\\Local\\Temp\\pystan_7tzpyj8y -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\pystan -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\pystan\\stan\\src -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\pystan\\stan\\lib\\stan_math -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\pystan\\stan\\lib\\stan_math\\lib\\eigen_3.3.3 -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\pystan\\stan\\lib\\stan_math\\lib\\boost_1.69.0 -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\pystan\\stan\\lib\\stan_math\\lib\\sundials_4.1.0\\include -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\lib\\site-packages\\numpy\\core\\include -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\include -IC:\\Users\\raviteja.kurva\\Anaconda3\\envs\\fb_prophet\\include -c C:\\Users\\RAVITE~1.KUR\\AppData\\Local\\Temp\\pystan_7tzpyj8y\\stanfit4anon_model_5944b02c79788fa0db5b3a93728ca2bf_3038101750471626612.cpp -o C:\\Users\\RAVITE~1.KUR\\AppData\\Local\\Temp\\pystan_7tzpyj8y\\users\\ravite~1.kur\\appdata\\local\\temp\\pystan_7tzpyj8y\\stanfit4anon_model_5944b02c79788fa0db5b3a93728ca2bf_3038101750471626612.o -O2 -ftemplate-depth-256 -Wno-unused-function -Wno-uninitialized -std=c++1y -D_hypot=hypot -pthread -fexceptions\" failed with exit status 1"
     ]
    }
   ],
   "source": [
    "import pystan\n",
    "model_code = 'parameters {real y;} model {y ~ normal(0,1);}'\n",
    "model = pystan.StanModel(model_code=model_code)  # this will take a minute\n",
    "y = model.sampling(n_jobs=1).extract()['y']\n",
    "y.mean()  # should be close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 128.20 Mb (19.4% reduction)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('input_files/Xbox_data.csv')\n",
    "data = reduce_mem_usage(data)\n",
    "\n",
    "\n",
    "#convert the sales date into datetime format \n",
    "\n",
    "data['Sales Date'] = pd.to_datetime(data['Sales Date'])\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "\n",
    "#data = (data.set_index(\"Sales Date\").groupby(['Reseller City','Super Division', 'Business Unit', pd.Grouper(freq='W')])[\"Rslr Sales Quantity\", \"Rslr Sales Amount\"].sum().astype(int).reset_index())\n",
    "data['black_week'] = np.where((data['Sales Date'].dt.month==11) & (data['Sales Date'].dt.day > 23), 1, 0)\n",
    "\n",
    "max_date = data['Sales Date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2021-12-31 00:00:00')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales Date</th>\n",
       "      <th>Rslr Sales Quantity</th>\n",
       "      <th>Store_names</th>\n",
       "      <th>Business Unit</th>\n",
       "      <th>Rslr Sales Amount</th>\n",
       "      <th>Super Division</th>\n",
       "      <th>Product Division</th>\n",
       "      <th>Reseller Postal Code</th>\n",
       "      <th>Reseller City</th>\n",
       "      <th>black_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Saturn Köln-Weiden</td>\n",
       "      <td>Game Pass PC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EDG Managed - Xbox Live</td>\n",
       "      <td>Xbox LIVE Gold</td>\n",
       "      <td>50858</td>\n",
       "      <td>Köln</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Saturn Köln-Weiden</td>\n",
       "      <td>Xbox Series S</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EDG Managed - Xbox Hardware</td>\n",
       "      <td>Xbox Console</td>\n",
       "      <td>50858</td>\n",
       "      <td>Köln</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Saturn Köln-Weiden</td>\n",
       "      <td>Xbox Series X</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EDG Managed - Xbox Hardware</td>\n",
       "      <td>Xbox Console</td>\n",
       "      <td>50858</td>\n",
       "      <td>Köln</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Fil. Media Markt Würzburg City</td>\n",
       "      <td>Game Pass PC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EDG Managed - Xbox Live</td>\n",
       "      <td>Xbox LIVE Gold</td>\n",
       "      <td>97070</td>\n",
       "      <td>Würzburg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Fil. Media Markt Würzburg City</td>\n",
       "      <td>Xbox Series S</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EDG Managed - Xbox Hardware</td>\n",
       "      <td>Xbox Console</td>\n",
       "      <td>97070</td>\n",
       "      <td>Würzburg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sales Date  Rslr Sales Quantity                     Store_names  \\\n",
       "0 2017-01-01                  0.0              Saturn Köln-Weiden   \n",
       "1 2017-01-01                  0.0              Saturn Köln-Weiden   \n",
       "2 2017-01-01                  0.0              Saturn Köln-Weiden   \n",
       "3 2017-01-01                  0.0  Fil. Media Markt Würzburg City   \n",
       "4 2017-01-01                  0.0  Fil. Media Markt Würzburg City   \n",
       "\n",
       "   Business Unit  Rslr Sales Amount               Super Division  \\\n",
       "0   Game Pass PC                0.0      EDG Managed - Xbox Live   \n",
       "1  Xbox Series S                0.0  EDG Managed - Xbox Hardware   \n",
       "2  Xbox Series X                0.0  EDG Managed - Xbox Hardware   \n",
       "3   Game Pass PC                0.0      EDG Managed - Xbox Live   \n",
       "4  Xbox Series S                0.0  EDG Managed - Xbox Hardware   \n",
       "\n",
       "  Product Division  Reseller Postal Code Reseller City  black_week  \n",
       "0   Xbox LIVE Gold                 50858          Köln           0  \n",
       "1     Xbox Console                 50858          Köln           0  \n",
       "2     Xbox Console                 50858          Köln           0  \n",
       "3   Xbox LIVE Gold                 97070      Würzburg           0  \n",
       "4     Xbox Console                 97070      Würzburg           0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sales Date              1789\n",
       "Rslr Sales Quantity      345\n",
       "Store_names              436\n",
       "Business Unit              3\n",
       "Rslr Sales Amount       8044\n",
       "Super Division             2\n",
       "Product Division           2\n",
       "Reseller Postal Code     409\n",
       "Reseller City            276\n",
       "black_week                 2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar as cal\n",
    "from datetime import *\n",
    "from dateutil.relativedelta import *\n",
    "import holidays\n",
    "\n",
    "def black_week(year):\n",
    "    black_friday = (datetime.date(datetime(year, 11, 1) + relativedelta(weekday=FR(+4)) - timedelta(days=4)), datetime.date(datetime(year, 11, 1) + relativedelta(weekday=FR(+4))+ timedelta(days=1)))\n",
    "    cyber_monday = datetime.date(datetime(year, 11, 1) + relativedelta(weekday=FR(+4))+ timedelta(days=3))\n",
    "\n",
    "    black_friday = pd.to_datetime(black_friday)\n",
    "    black_friday =pd.date_range(black_friday[0], black_friday[1], freq ='d')\n",
    "\n",
    "    cyber_monday = pd.to_datetime(cyber_monday)\n",
    "    #cyber_monday = list(cyber_monday)\n",
    "\n",
    "\n",
    "    return black_friday, cyber_monday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## create the test_dataframe \n",
    "\n",
    "test_data = create_store_test(data, max_date)\n",
    "\n",
    "test_data['Sales Date'] = pd.to_datetime(test_data['Sales Date'])\n",
    "test_data['black_week'] = np.where((test_data['Sales Date'].dt.month==11) & (test_data['Sales Date'].dt.day > 23), 1, 0)\n",
    "\n",
    "data = pd.concat([data, test_data])\n",
    "\n",
    "data['Sales Date'] = pd.to_datetime(data['Sales Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2022-01-14 00:00:00')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Sales Date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07 min: Lags\n",
      "root\n",
      " |-- ds: timestamp (nullable = true)\n",
      " |-- Rslr_Sales_Qunatity: double (nullable = true)\n",
      " |-- Store_names: string (nullable = true)\n",
      " |-- Business_Unit: string (nullable = true)\n",
      " |-- Rslr Sales Amount: double (nullable = true)\n",
      " |-- Super_Division: string (nullable = true)\n",
      " |-- Product_Division: string (nullable = true)\n",
      " |-- Reseller Postal Code: long (nullable = true)\n",
      " |-- Reseller_City: string (nullable = true)\n",
      " |-- black_week: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "spark = SparkSession.builder\\\n",
    "        .appName('Xbox_store_predictions') \\\n",
    "        .master('local[6]') \\\n",
    "        .config('spark.sql.execution.arrow.pyspark.enabled', True) \\\n",
    "        .config('spark.sql.execution.arrow.enabled', True) \\\n",
    "        .config('spark.sql.session.timeZone', 'UTC') \\\n",
    "        .config('spark.executor.memory','6G') \\\n",
    "        .config('spark.driver.memory','8G') \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
    "        .getOrCreate()\n",
    "         #.config('spark.sql.repl.eagerEval.enabled', True) \\\n",
    "         #.config('spark.ui.showConsoleProgress', True) \\\n",
    "         #.config('spark.default.parallelism', 1308) \\\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "sdf = spark.createDataFrame(data)\n",
    "print('%0.2f min: Lags' % ((time.time() - start_time) / 60))\n",
    "sdf = sdf.withColumnRenamed('Sales Date', 'ds')\\\n",
    "        .withColumnRenamed('Super Division', 'Super_Division')\\\n",
    "        .withColumnRenamed('Product Division', 'Product_Division')\\\n",
    "        .withColumnRenamed('Rslr Sales Quantity', 'Rslr_Sales_Qunatity')\\\n",
    "        .withColumnRenamed('Reseller City', 'Reseller_City')\\\n",
    "        .withColumnRenamed('Business Unit', 'Business_Unit')\n",
    "\n",
    "sdf.printSchema()\n",
    "\n",
    "sdf.createOrReplaceTempView('sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.maxResultSize', '2G'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.memory', '8G'),\n",
       " ('spark.master', 'local[6]'),\n",
       " ('spark.driver.host', 'Grogu.mshome.net'),\n",
       " ('spark.app.startTime', '1642587070032'),\n",
       " ('spark.executor.memory', '6G'),\n",
       " ('spark.driver.port', '58748'),\n",
       " ('spark.app.id', 'local-1642587071556'),\n",
       " ('spark.sql.execution.arrow.pyspark.enabled', 'True'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.name', 'Xbox_store_predictions'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.sql.session.timeZone', 'UTC'),\n",
       " ('spark.sql.execution.arrow.enabled', 'True'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-------------+--------+\n",
      "|         Store_names|Reseller_City|Business_Unit|count(1)|\n",
      "+--------------------+-------------+-------------+--------+\n",
      "|  Media Markt Aachen|       Aachen| Game Pass PC|    1803|\n",
      "|       Saturn Aachen|       Aachen| Game Pass PC|    1803|\n",
      "|  Media Markt Aachen|       Aachen|Xbox Series S|    1803|\n",
      "|       Saturn Aachen|       Aachen|Xbox Series S|    1803|\n",
      "|       Saturn Aachen|       Aachen|Xbox Series X|    1803|\n",
      "|  Media Markt Aachen|       Aachen|Xbox Series X|    1803|\n",
      "|   Media Markt Aalen|        Aalen| Game Pass PC|    1803|\n",
      "|   Media Markt Aalen|        Aalen|Xbox Series S|    1803|\n",
      "|   Media Markt Aalen|        Aalen|Xbox Series X|    1742|\n",
      "|Media Markt Berli...|  Ahrensfelde| Game Pass PC|    1803|\n",
      "|Media Markt Berli...|  Ahrensfelde|Xbox Series S|    1803|\n",
      "|Media Markt Berli...|  Ahrensfelde|Xbox Series X|    1742|\n",
      "|Media Markt Albstadt|     Albstadt| Game Pass PC|    1803|\n",
      "|Media Markt Albstadt|     Albstadt|Xbox Series S|    1803|\n",
      "|Media Markt Albstadt|     Albstadt|Xbox Series X|    1803|\n",
      "|   Media Markt Alzey|        Alzey| Game Pass PC|    1803|\n",
      "|   Media Markt Alzey|        Alzey|Xbox Series S|    1803|\n",
      "|   Media Markt Alzey|        Alzey|Xbox Series X|    1803|\n",
      "|  Media Markt Amberg|       Amberg| Game Pass PC|    1803|\n",
      "|  Media Markt Amberg|       Amberg|Xbox Series S|    1803|\n",
      "+--------------------+-------------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [Sales Date#0 AS ds#20, Rslr Sales Quantity#1 AS Rslr_Sales_Qunatity#53, Store_names#2, Business Unit#3 AS Business_Unit#75, Rslr Sales Amount#4, Super Division#5 AS Super_Division#31, Product Division#6 AS Product_Division#42, Reseller Postal Code#7L, Reseller City#8 AS Reseller_City#64, black_week#9]\n",
      "+- *(1) Scan ExistingRDD arrow[Sales Date#0,Rslr Sales Quantity#1,Store_names#2,Business Unit#3,Rslr Sales Amount#4,Super Division#5,Product Division#6,Reseller Postal Code#7L,Reseller City#8,black_week#9]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Store_names, Reseller_City,  Business_Unit, count(*) from sales group by Store_names, Reseller_City, Business_Unit order by Reseller_City, Business_Unit\").show()\n",
    "\n",
    "sql = 'SELECT Store_names, Reseller_City, Super_Division, Product_Division, Business_Unit, black_week, ds, sum(Rslr_Sales_Qunatity) as y FROM sales GROUP BY Store_names, Reseller_City, Super_Division, Product_Division, Business_Unit, black_week, ds ORDER BY Store_names, Reseller_City,  Super_Division,  Business_Unit, ds'\n",
    "\n",
    "sdf.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+--------------------+----------------+-------------+----------+-------------------+---+\n",
      "|        Store_names|Reseller_City|      Super_Division|Product_Division|Business_Unit|black_week|                 ds|  y|\n",
      "+-------------------+-------------+--------------------+----------------+-------------+----------+-------------------+---+\n",
      "| Saturn Köln-Weiden|         Köln|EDG Managed - Xbo...|    Xbox Console|Xbox Series S|         0|2017-01-01 00:00:00|0.0|\n",
      "| Saturn Köln-Weiden|         Köln|EDG Managed - Xbo...|    Xbox Console|Xbox Series S|         0|2017-01-02 00:00:00|6.0|\n",
      "| Saturn Köln-Weiden|         Köln|EDG Managed - Xbo...|    Xbox Console|Xbox Series S|         0|2017-01-03 00:00:00|5.0|\n",
      "| Saturn Köln-Weiden|         Köln|EDG Managed - Xbo...|    Xbox Console|Xbox Series S|         0|2017-01-04 00:00:00|0.0|\n",
      "| Saturn Köln-Weiden|         Köln|EDG Managed - Xbo...|    Xbox Console|Xbox Series S|         0|2017-01-05 00:00:00|0.0|\n",
      "| Saturn Köln-Weiden|         Köln|EDG Managed - Xbo...|    Xbox Console|Xbox Series S|         0|2017-01-06 00:00:00|0.0|\n",
      "| Saturn Köln-Weiden|         Köln|EDG Managed - Xbo...|    Xbox Console|Xbox Series S|         0|2017-01-07 00:00:00|0.0|\n",
      "| Saturn Köln-Weiden|         Köln|EDG Managed - Xbo...|    Xbox Console|Xbox Series S|         0|2017-01-08 00:00:00|0.0|\n",
      "| Saturn Köln-Weiden|         Köln|EDG Managed - Xbo...|    Xbox Console|Xbox Series S|         0|2017-01-09 00:00:00|0.0|\n",
      "| Saturn Köln-Weiden|         Köln|EDG Managed - Xbo...|    Xbox Console|Xbox Series S|         0|2017-01-10 00:00:00|0.0|\n",
      "| Saturn Köln-Weiden|         Köln|EDG Managed - Xbo...|    Xbox Console|Xbox Series S|         0|2017-01-11 00:00:00|1.0|\n",
      "| Saturn Köln-Weiden|         Köln|EDG Managed - Xbo...|    Xbox Console|Xbox Series S|         0|2017-01-12 00:00:00|0.0|\n",
      "| Saturn Köln-Weiden|         Köln|EDG Managed - Xbo...|    Xbox Console|Xbox Series S|         0|2017-01-13 00:00:00|0.0|\n",
      "| Saturn Köln-Weiden|         Köln|EDG Managed - Xbo...|    Xbox Console|Xbox Series S|         0|2017-01-14 00:00:00|1.0|\n",
      "| Saturn Köln-Weiden|         Köln|EDG Managed - Xbo...|    Xbox Console|Xbox Series S|         0|2017-01-15 00:00:00|0.0|\n",
      "| Saturn Köln-Weiden|         Köln|EDG Managed - Xbo...|    Xbox Console|Xbox Series S|         0|2017-01-16 00:00:00|1.0|\n",
      "| Saturn Köln-Weiden|         Köln|EDG Managed - Xbo...|    Xbox Console|Xbox Series S|         0|2017-01-17 00:00:00|0.0|\n",
      "| Saturn Köln-Weiden|         Köln|EDG Managed - Xbo...|    Xbox Console|Xbox Series S|         0|2017-01-18 00:00:00|0.0|\n",
      "| Saturn Köln-Weiden|         Köln|EDG Managed - Xbo...|    Xbox Console|Xbox Series S|         0|2017-01-19 00:00:00|0.0|\n",
      "| Saturn Köln-Weiden|         Köln|EDG Managed - Xbo...|    Xbox Console|Xbox Series S|         0|2017-01-20 00:00:00|0.0|\n",
      "+-------------------+-------------+--------------------+----------------+-------------+----------+-------------------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "InMemoryTableScan [Store_names#2, Reseller_City#64, Super_Division#31, Product_Division#42, Business_Unit#75, black_week#9, ds#20, y#163]\n",
      "   +- InMemoryRelation [Store_names#2, Reseller_City#64, Super_Division#31, Product_Division#42, Business_Unit#75, black_week#9, ds#20, y#163], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- Exchange hashpartitioning(Store_names#2, Business_Unit#75, 6), false, [id=#100]\n",
      "            +- *(3) Sort [Store_names#2 ASC NULLS FIRST, Reseller_City#64 ASC NULLS FIRST, Super_Division#31 ASC NULLS FIRST, Business_Unit#75 ASC NULLS FIRST, ds#20 ASC NULLS FIRST], true, 0\n",
      "               +- Exchange rangepartitioning(Store_names#2 ASC NULLS FIRST, Reseller_City#64 ASC NULLS FIRST, Super_Division#31 ASC NULLS FIRST, Business_Unit#75 ASC NULLS FIRST, ds#20 ASC NULLS FIRST, 200), true, [id=#96]\n",
      "                  +- *(2) HashAggregate(keys=[Store_names#2, Reseller_City#64, Super_Division#31, Product_Division#42, Business_Unit#75, black_week#9, ds#20], functions=[sum(Rslr_Sales_Qunatity#53)])\n",
      "                     +- Exchange hashpartitioning(Store_names#2, Reseller_City#64, Super_Division#31, Product_Division#42, Business_Unit#75, black_week#9, ds#20, 200), true, [id=#92]\n",
      "                        +- *(1) HashAggregate(keys=[Store_names#2, Reseller_City#64, Super_Division#31, Product_Division#42, Business_Unit#75, black_week#9, ds#20], functions=[partial_sum(Rslr_Sales_Qunatity#53)])\n",
      "                           +- *(1) Project [Sales Date#0 AS ds#20, Rslr Sales Quantity#1 AS Rslr_Sales_Qunatity#53, Store_names#2, Business Unit#3 AS Business_Unit#75, Super Division#5 AS Super_Division#31, Product Division#6 AS Product_Division#42, Reseller City#8 AS Reseller_City#64, black_week#9]\n",
      "                              +- *(1) Scan ExistingRDD arrow[Sales Date#0,Rslr Sales Quantity#1,Store_names#2,Business Unit#3,Rslr Sales Amount#4,Super Division#5,Product Division#6,Reseller Postal Code#7L,Reseller City#8,black_week#9]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### make partitions of data based on number of cores in the local system\n",
    "sdf.rdd.getNumPartitions()\n",
    "\n",
    "spark.sql(sql).show()\n",
    "\n",
    "store_part = (spark.sql(sql).repartition(spark.sparkContext.defaultParallelism, ['Store_names','Business_Unit'])).cache()\n",
    "store_part.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### make the resultant schema\n",
    "result_schema =StructType([\n",
    "    StructField('ds',TimestampType()),\n",
    "    StructField('Store_names',StringType()),\n",
    "    StructField('Reseller_City',StringType()),\n",
    "    StructField('Super_Division',StringType()),\n",
    "    StructField('Product_Division',StringType()),\n",
    "    StructField('Business_Unit',StringType()),\n",
    "    StructField('y',DoubleType()),\n",
    "    StructField('yhat',DoubleType()),\n",
    "    StructField('yhat_upper',DoubleType()),\n",
    "    StructField('yhat_lower',DoubleType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create the holiday dataframe \n",
    "\n",
    "lockdown1 = pd.date_range('2020-03-22', '2020-05-03', freq ='d').to_list()\n",
    "lockdown2 = pd.date_range('2020-12-13', '2021-03-07', freq ='d').to_list()\n",
    "lockdown = lockdown1+lockdown2\n",
    "\n",
    "\n",
    "lock_down = pd.DataFrame({\n",
    "    'holiday': 'lock_down',\n",
    "    'ds' : pd.to_datetime(lockdown)})\n",
    "\n",
    "\n",
    "sundays = pd.date_range('2017-01-01', '2022-01-14', freq ='w').to_list()\n",
    "\n",
    "\n",
    "sundays = pd.DataFrame({\n",
    "    'holiday': 'sundays',\n",
    "    'ds' : pd.to_datetime(sundays)})\n",
    "\n",
    "\n",
    "\n",
    "holiday = pd.concat([lock_down, sundays])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### city-wise prophet function \n",
    "@pandas_udf( result_schema, PandasUDFType.GROUPED_MAP )\n",
    "def forecast_sales( store_pd):\n",
    "    \n",
    "    model = Prophet(interval_width=0.95, holidays = holiday)\n",
    "    model.add_country_holidays(country_name='DE')\n",
    "    model.add_regressor('black_week')\n",
    "    \n",
    "    train = store_pd[store_pd['ds']<='2021-12-31']\n",
    "    future_pd = store_pd[store_pd['ds']>'2021-12-31'].set_index('ds')\n",
    "\n",
    "    train['date_index'] = train['ds']\n",
    "    train['date_index'] = pd.to_datetime(train['date_index'])\n",
    "    train = train.set_index('date_index')\n",
    "\n",
    "\n",
    "    model.fit(train[['ds', 'y', 'black_week']])\n",
    "    future = model.make_future_dataframe(periods=14, freq='d')\n",
    "    future['black_week'] = np.where((future['ds'].dt.month==11) & (future['ds'].dt.day > 23), 1, 0)\n",
    "\n",
    "    #future['black_week'] = future['ds'].apply(black_week)\n",
    "    #future['promos'] = future['ds'].apply(promos)\n",
    "\n",
    "    forecast_pd = model.predict(future[['ds', 'black_week']])  \n",
    "\n",
    "\n",
    "    #forecast_pd = model.predict(future_pd[['ds', 'black_week', 'EOL_Promos']])  \n",
    "\n",
    "    f_pd = forecast_pd[ ['ds','yhat', 'yhat_upper', 'yhat_lower'] ].set_index('ds')\n",
    "\n",
    "    #store_pd = store_pd.filter(store_pd['ds']<'2021-10-01 00:00:00')\n",
    "    st_pd = store_pd[[ 'ds', 'Store_names', 'Reseller_City', 'Super_Division', 'Product_Division', 'Business_Unit','y']].set_index('ds')\n",
    "    #st_pd = pd.concat([st_pd1, st_pd2])\n",
    "\n",
    "    results_pd = f_pd.join( st_pd, how='left' )\n",
    "    results_pd.reset_index(level=0, inplace=True)\n",
    "\n",
    "    #results_pd[['Reseller_City','Business_Unit']] = future_pd[['Reseller_City','Business_Unit']].iloc[0]\n",
    "\n",
    "    return results_pd[ ['ds', 'Store_names', 'Reseller_City', 'Super_Division', 'Product_Division', 'Business_Unit','y', 'yhat', 'yhat_upper', 'yhat_lower'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 min: Lags\n",
      "== Physical Plan ==\n",
      "InMemoryTableScan [ds#351, Store_names#352, Reseller_City#353, Super_Division#354, Product_Division#355, Business_Unit#356, y#357, yhat#358, yhat_upper#359, yhat_lower#360, training_date#371]\n",
      "   +- InMemoryRelation [ds#351, Store_names#352, Reseller_City#353, Super_Division#354, Product_Division#355, Business_Unit#356, y#357, yhat#358, yhat_upper#359, yhat_lower#360, training_date#371], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(2) Project [ds#351, Store_names#352, Reseller_City#353, Super_Division#354, Product_Division#355, Business_Unit#356, y#357, yhat#358, yhat_upper#359, yhat_lower#360, 19011 AS training_date#371]\n",
      "            +- FlatMapGroupsInPandas [Store_names#2, Business_Unit#75], forecast_sales(Store_names#2, Reseller_City#64, Super_Division#31, Product_Division#42, Business_Unit#75, black_week#9, ds#20, y#163), [ds#351, Store_names#352, Reseller_City#353, Super_Division#354, Product_Division#355, Business_Unit#356, y#357, yhat#358, yhat_upper#359, yhat_lower#360]\n",
      "               +- *(1) Sort [Store_names#2 ASC NULLS FIRST, Business_Unit#75 ASC NULLS FIRST], false, 0\n",
      "                  +- InMemoryTableScan [Store_names#2, Business_Unit#75, Store_names#2, Reseller_City#64, Super_Division#31, Product_Division#42, Business_Unit#75, black_week#9, ds#20, y#163]\n",
      "                        +- InMemoryRelation [Store_names#2, Reseller_City#64, Super_Division#31, Product_Division#42, Business_Unit#75, black_week#9, ds#20, y#163], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                              +- Exchange hashpartitioning(Store_names#2, Business_Unit#75, 6), false, [id=#100]\n",
      "                                 +- *(3) Sort [Store_names#2 ASC NULLS FIRST, Reseller_City#64 ASC NULLS FIRST, Super_Division#31 ASC NULLS FIRST, Business_Unit#75 ASC NULLS FIRST, ds#20 ASC NULLS FIRST], true, 0\n",
      "                                    +- Exchange rangepartitioning(Store_names#2 ASC NULLS FIRST, Reseller_City#64 ASC NULLS FIRST, Super_Division#31 ASC NULLS FIRST, Business_Unit#75 ASC NULLS FIRST, ds#20 ASC NULLS FIRST, 200), true, [id=#96]\n",
      "                                       +- *(2) HashAggregate(keys=[Store_names#2, Reseller_City#64, Super_Division#31, Product_Division#42, Business_Unit#75, black_week#9, ds#20], functions=[sum(Rslr_Sales_Qunatity#53)])\n",
      "                                          +- Exchange hashpartitioning(Store_names#2, Reseller_City#64, Super_Division#31, Product_Division#42, Business_Unit#75, black_week#9, ds#20, 200), true, [id=#92]\n",
      "                                             +- *(1) HashAggregate(keys=[Store_names#2, Reseller_City#64, Super_Division#31, Product_Division#42, Business_Unit#75, black_week#9, ds#20], functions=[partial_sum(Rslr_Sales_Qunatity#53)])\n",
      "                                                +- *(1) Project [Sales Date#0 AS ds#20, Rslr Sales Quantity#1 AS Rslr_Sales_Qunatity#53, Store_names#2, Business Unit#3 AS Business_Unit#75, Super Division#5 AS Super_Division#31, Product Division#6 AS Product_Division#42, Reseller City#8 AS Reseller_City#64, black_week#9]\n",
      "                                                   +- *(1) Scan ExistingRDD arrow[Sales Date#0,Rslr Sales Quantity#1,Store_names#2,Business Unit#3,Rslr Sales Amount#4,Super Division#5,Product Division#6,Reseller Postal Code#7L,Reseller City#8,black_week#9]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "results = (\n",
    "    store_part\n",
    "    .groupBy(['Store_names','Business_Unit'])\n",
    "    .apply(forecast_sales)\n",
    "    .withColumn('training_date', current_date() )\n",
    "    )\n",
    "\n",
    "### cache the results \n",
    "start_time = time.time()\n",
    "results.cache()\n",
    "print('%0.2f min: Lags' % ((time.time() - start_time) / 60))\n",
    "\n",
    "results.explain()\n",
    "results = results.coalesce(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o147.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 1018, Grogu.mshome.net, executor driver): java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(Unknown Source)\r\n\tat java.net.SocketInputStream.read(Unknown Source)\r\n\tat java.io.BufferedInputStream.fill(Unknown Source)\r\n\tat java.io.BufferedInputStream.read(Unknown Source)\r\n\tat java.io.DataInputStream.readInt(Unknown Source)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:86)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:311)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.CoalescedRDD.$anonfun$compute$1(CoalescedRDD.scala:99)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(Unknown Source)\r\n\tat java.net.SocketInputStream.read(Unknown Source)\r\n\tat java.io.BufferedInputStream.fill(Unknown Source)\r\n\tat java.io.BufferedInputStream.read(Unknown Source)\r\n\tat java.io.DataInputStream.readInt(Unknown Source)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:86)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:311)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.CoalescedRDD.$anonfun$compute$1(CoalescedRDD.scala:99)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pyspark\\lib\\site-packages\\pyspark\\sql\\dataframe.py:440\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \n\u001b[0;32m    408\u001b[0m \u001b[38;5;124;03m:param n: Number of rows to show.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;124;03m name | Bob\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;28mint\u001b[39m(truncate), vertical))\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pyspark\\lib\\site-packages\\py4j\\java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pyspark\\lib\\site-packages\\pyspark\\sql\\utils.py:128\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    130\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pyspark\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o147.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 1018, Grogu.mshome.net, executor driver): java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(Unknown Source)\r\n\tat java.net.SocketInputStream.read(Unknown Source)\r\n\tat java.io.BufferedInputStream.fill(Unknown Source)\r\n\tat java.io.BufferedInputStream.read(Unknown Source)\r\n\tat java.io.DataInputStream.readInt(Unknown Source)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:86)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:311)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.CoalescedRDD.$anonfun$compute$1(CoalescedRDD.scala:99)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(Unknown Source)\r\n\tat java.net.SocketInputStream.read(Unknown Source)\r\n\tat java.io.BufferedInputStream.fill(Unknown Source)\r\n\tat java.io.BufferedInputStream.read(Unknown Source)\r\n\tat java.io.DataInputStream.readInt(Unknown Source)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:86)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:311)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.CoalescedRDD.$anonfun$compute$1(CoalescedRDD.scala:99)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### run the spark by store-wise\n",
    "\n",
    "\n",
    "### convert the result from sparkdataframe to panadas datafrme \n",
    "start_time = time.time()\n",
    "final_df = results.toPandas()\n",
    "\n",
    "print('%0.2f min: Lags' % ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['yhat'] = np.where(final_df['yhat']<0, 0, final_df['yhat'])\n",
    "final_df['yhat_upper'] = np.where(final_df['yhat_upper']<0, 0, final_df['yhat_upper'])\n",
    "final_df['yhat_lower'] = np.where(final_df['yhat_lower']<0, 0, final_df['yhat_lower'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.drop_duplicates()\n",
    "final_df.to_csv(r'output_files/Xbox_store_historic.csv', index=False)\n",
    "\n",
    "predicted_data = final_df.loc[final_df['ds']> '2021-12-31']\n",
    "\n",
    "predicted_data = round_target_column(predicted_data, 'yhat')\n",
    "predicted_data = round_target_column(predicted_data, 'yhat_lower')\n",
    "predicted_data = round_target_column(predicted_data, 'yhat_upper')\n",
    "\n",
    "predicted_data.to_csv(r'output_files/Xbox_store.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2c74751c2b923e2362cfe1d79031e9569c6f81d163515e88ad3756b52df25df3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
